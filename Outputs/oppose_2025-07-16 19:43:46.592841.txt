While some studies suggest emergent reasoning in large language models, a growing body of research indicates that these abilities might be overstated and can be attributed to alternative mechanisms such as memorization, pattern matching, and prompt engineering.

Several papers highlight the role of memorization. "Reason to Rote: Rethinking Memorization in Reasoning" and "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation" directly challenge the notion of genuine reasoning, suggesting that LLMs often rely on memorized information rather than true reasoning. "Analyzing Memorization in Large Language Models through the Lens of Model Attribution" further investigates how memorization manifests in LLMs. "Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs" attempts to disentangle the effects of in-context learning from memorization, suggesting that memorization plays a significant role.

Furthermore, the apparent reasoning abilities of LLMs can be heavily influenced by prompt engineering. "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications" and "Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs" demonstrate the importance of carefully crafted prompts in eliciting desired responses from LLMs. This raises the possibility that observed reasoning is not an inherent property of the model but rather a result of clever prompt design.

Therefore, while LLMs may exhibit impressive performance on certain reasoning tasks, it is crucial to consider alternative explanations such as memorization and prompt engineering before attributing these abilities to genuine emergent reasoning. Further research is needed to disentangle these factors and gain a deeper understanding of the true capabilities and limitations of large language models.