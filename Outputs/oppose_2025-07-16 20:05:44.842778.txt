Several papers challenge the claim that foundation models exhibit emergent reasoning, suggesting that observed reasoning capabilities may be attributed to simpler mechanisms or shortcuts.

"Break the Chain: Large Language Models Can be Shortcut Reasoners" directly argues that LLMs can act as shortcut reasoners. "Why Machine Reading Comprehension Models Learn Shortcuts?" investigates shortcut learning in machine reading comprehension, a related task. "Shortcut Learning of Large Language Models in Natural Language Understanding" explores shortcut learning in LLMs specifically within the context of natural language understanding. "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models" examines factual shortcuts in knowledge editing, highlighting potential limitations in reasoning. "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts" proposes a model of reasoning that relies on dynamic shortcuts.

Furthermore, some papers point out limitations in the reasoning abilities of LLMs. "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?" questions the ability of LLMs to perform complex logical reasoning. "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models" aims to provide a deeper understanding of the reasoning capabilities of LLMs, implicitly acknowledging that current understanding is incomplete and that limitations may exist.